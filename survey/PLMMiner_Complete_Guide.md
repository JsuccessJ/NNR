# PLMMiner ëª¨ë¸ í†µí•© ê°€ì´ë“œ

**ë‚ ì§œ**: 2025-12-20
**ëª¨ë¸**: PLMMiner (News Encoder) + MINER (User Encoder)
**ë°ì´í„°ì…‹**: MIND (Microsoft News Dataset)

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹¤í–‰ ëª…ë ¹ì–´](#1-ì‹¤í–‰-ëª…ë ¹ì–´)
2. [Git ë³€ê²½ì‚¬í•­ ìš”ì•½](#2-git-ë³€ê²½ì‚¬í•­-ìš”ì•½)
3. [ëª¨ë¸ ì•„í‚¤í…ì²˜](#3-ëª¨ë¸-ì•„í‚¤í…ì²˜)

---

## 1. ì‹¤í–‰ ëª…ë ¹ì–´

### ê¸°ë³¸ í›ˆë ¨

```bash
python main.py \
    --news_encoder=PLMMiner \
    --user_encoder=MINER \
    --dataset=small \
    --mode=train \
    --use_category_glove \
    --batch_size=32 \
    --epoch=5
```

### ì£¼ìš” íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¸°ë³¸ê°’ | ê¶Œì¥ê°’ |
|---------|------|--------|--------|
| `--news_encoder` | ë‰´ìŠ¤ ì¸ì½”ë” ì„ íƒ | - | `PLMMiner` |
| `--user_encoder` | ìœ ì € ì¸ì½”ë” ì„ íƒ | - | `MINER` |
| `--dataset` | ë°ì´í„°ì…‹ í¬ê¸° | `small` | `small/200k/large` |
| `--use_category_glove` | GloVe ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” | False | ì‚¬ìš© ê¶Œì¥ |
| `--plm_type` | PLM ëª¨ë¸ íƒ€ì… | `bert` | `bert/roberta` |
| `--plm_model_name` | PLM ëª¨ë¸ ì´ë¦„ | `bert-base-uncased` | - |
| `--plm_frozen_layers` | PLM ë™ê²° ë ˆì´ì–´ ìˆ˜ | 10 | 10 (ìƒìœ„ 2ê°œì¸µë§Œ í•™ìŠµ) |
| `--plm_lr` | PLM learning rate | 1e-5 | 1e-5 |
| `--num_interest_vectors` | Interest vector ê°œìˆ˜ (K) | 32 | 32 |
| `--context_code_dim` | Context code ì°¨ì› | 200 | 200 |
| `--category_aware_lambda` | Category similarity ê°€ì¤‘ì¹˜ | 0.5 | 0.5 |
| `--miner_aggregation` | Score ì§‘ê³„ ë°©ì‹ | `weighted` | `weighted/max/mean` |
| `--disagreement_beta` | Disagreement loss ê°€ì¤‘ì¹˜ | 0.8 | 0.8 |

### Dev/Test í‰ê°€

```bash
# Dev í‰ê°€
python main.py \
    --mode=dev \
    --news_encoder=PLMMiner \
    --user_encoder=MINER \
    --dataset=small \
    --dev_model_path=best_model/small/PLMMiner-MINER/#2/PLMMiner-MINER

# Test í‰ê°€
python main.py \
    --mode=test \
    --news_encoder=PLMMiner \
    --user_encoder=MINER \
    --dataset=small \
    --test_model_path=best_model/small/PLMMiner-MINER/#2/PLMMiner-MINER
```

---

## 2. Git ë³€ê²½ì‚¬í•­ ìš”ì•½

### íŒŒì¼ë³„ ë³€ê²½ ë‚´ìš©

#### 2.1 MIND_corpus.py

**ìœ„ì¹˜**: Line 482-486

**ë³€ê²½ ì „:**
```python
                self.news_plm_title_ids = data['title_ids']
                self.news_plm_title_masks = data['title_masks']

        print(f'PLM tokenization completed. Shape: {self.news_plm_title_ids.shape}')
```

**ë³€ê²½ í›„:**
```python
                self.news_plm_title_ids = data['title_ids']
                self.news_plm_title_masks = data['title_masks']

        print(f'PLM preprocessing completed. Replacing news_title_text with PLM tokenized data.')
        # PLM ë°ì´í„°ë¥¼ ê¸°ë³¸ ë°ì´í„°ë¡œ êµì²´ (ê¸°ì¡´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ì™€ í˜¸í™˜ì„± ìœ ì§€)
        self.news_title_text = self.news_plm_title_ids
        self.news_title_mask = self.news_plm_title_masks

        print(f'PLM tokenization completed. Shape: {self.news_plm_title_ids.shape}')
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- PLM í† í°í™” ë°ì´í„°ë¥¼ ê¸°ì¡´ í•„ë“œ(`news_title_text`, `news_title_mask`)ì— ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ë°ì´í„° ë¡œë” í˜¸í™˜ì„± ìœ ì§€
- ê¸°ì¡´ Word2Vec ê¸°ë°˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì¬ì‚¬ìš©í•˜ë©´ì„œ PLM ë°ì´í„°ë¥¼ íˆ¬ëª…í•˜ê²Œ í†µí•©

---

#### 2.2 config.py

**ìœ„ì¹˜**: Line 15-16

**ë³€ê²½ ì „:**
```python
parser.add_argument('--news_encoder', type=str, default='CNE',
    choices=['CNE', 'CNN', 'MHSA', ..., 'NAML_Title', ...])
parser.add_argument('--user_encoder', type=str, default='SUE',
    choices=['SUE', 'LSTUR', 'MHSA', ..., 'SUE_wo_HCA'])
```

**ë³€ê²½ í›„:**
```python
parser.add_argument('--news_encoder', type=str, default='CNE',
    choices=['CNE', 'CNN', ..., 'PLMNAML', 'PLMNRMS', 'PLMMiner', ...])
parser.add_argument('--user_encoder', type=str, default='SUE',
    choices=['SUE', 'LSTUR', ..., 'MINER', 'SUE_wo_HCA'])
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- `PLMMiner`, `MINER` ì¸ì½”ë” ì„ íƒ ì˜µì…˜ ì¶”ê°€

---

**ìœ„ì¹˜**: Line 84-92 (MINER í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€)

**ë³€ê²½ ì „:**
```python
parser.add_argument('--use_plm_news_encoder', action='store_true')

        self.attribute_dict = dict(vars(parser.parse_args()))
```

**ë³€ê²½ í›„:**
```python
parser.add_argument('--use_plm_news_encoder', action='store_true')
        # MINER-specific parameters
        parser.add_argument('--num_interest_vectors', type=int, default=32)
        parser.add_argument('--context_code_dim', type=int, default=200)
        parser.add_argument('--disagreement_beta', type=float, default=0.8)
        parser.add_argument('--miner_aggregation', type=str, default='weighted', choices=['max', 'mean', 'weighted'])
        parser.add_argument('--category_aware_lambda', type=float, default=0.5)
        parser.add_argument('--use_category_glove', action='store_true')

        self.attribute_dict = dict(vars(parser.parse_args()))

        # PLM ê¸°ë°˜ ë‰´ìŠ¤ ì¸ì½”ë” ìë™ ì„¤ì •
        if self.news_encoder in ['PLMNAML', 'PLMNRMS', 'PLMMiner']:
            self.use_plm_news_encoder = True
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **MINER í•µì‹¬ íŒŒë¼ë¯¸í„° 6ê°œ ì¶”ê°€**: Interest vectors ê°œìˆ˜(K=32), Context dimension(200), Disagreement loss(Î²=0.8), Aggregation ë°©ì‹(weighted), Category-aware lambda(Î»=0.5), GloVe ì´ˆê¸°í™” í”Œë˜ê·¸
- **ìë™ ì„¤ì • ë¡œì§**: PLM ê¸°ë°˜ ì¸ì½”ë” ì„ íƒ ì‹œ `use_plm_news_encoder` ìë™ í™œì„±í™”ë¡œ ì¤‘ë³µ í”Œë˜ê·¸ ë¶ˆí•„ìš”

---

#### 2.3 main.py

**ìœ„ì¹˜**: Line 13-15, 38, 52

**ë³€ê²½ ì „:**
```python
model = Model(config)
```

**ë³€ê²½ í›„:**
```python
model = Model(config, mind_corpus.category_dict)
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- PLMMinerê°€ GloVeë¡œ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì´ˆê¸°í™”í•  ë•Œ í•„ìš”í•œ `category_dict` (ì¹´í…Œê³ ë¦¬ ì´ë¦„â†’ID ë§¤í•‘) ì „ë‹¬
- ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ì˜ˆ: 'sports', 'entertainment')ì„ GloVe ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë©”íƒ€ë°ì´í„° ì œê³µ

---

#### 2.4 model.py

**ìœ„ì¹˜**: Line 10-28

**ë³€ê²½ ì „:**
```python
class Model(nn.Module):
    def __init__(self, config: Config):
        super(Model, self).__init__()
        if config.use_plm_news_encoder:
            if config.new_encoder == 'NAML':
                self.news_encoder = newsEncoders.PLMNAML(config)
            elif config.news_encoder == 'NRMS':
                self.news_encoder = newsEncoders.PLMNRMS(config)
        if config.news_encoder == 'CNE':
```

**ë³€ê²½ í›„:**
```python
class Model(nn.Module):
    def __init__(self, config: Config, category_dict: dict = None):
        super(Model, self).__init__()

        if config.news_encoder == 'PLMNAML':
            self.news_encoder = newsEncoders.PLMNAML(config)
        elif config.news_encoder == 'PLMNRMS':
            self.news_encoder = newsEncoders.PLMNRMS(config)
        elif config.news_encoder == 'PLMMiner':
            assert category_dict is not None, 'PLMMiner requires category_dict'
            self.news_encoder = newsEncoders.PLMMiner(config, category_dict)
        elif config.news_encoder == 'CNE':
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **ë¶„ê¸° êµ¬ì¡° ê°œì„ **: `use_plm_news_encoder` í”Œë˜ê·¸ ê¸°ë°˜ ifë¬¸ì„ ëª…ì‹œì  ì¸ì½”ë”ëª… ê¸°ë°˜ if/elifë¡œ ë³€ê²½í•˜ì—¬ ì½”ë“œ ëª…í™•ì„± í–¥ìƒ
- **PLMMiner ì¶”ê°€**: `category_dict` assertionìœ¼ë¡œ GloVe ì´ˆê¸°í™”ì— í•„ìš”í•œ ë°ì´í„° ë³´ì¥

---

**ìœ„ì¹˜**: Line 75-76 (MINER ìœ ì € ì¸ì½”ë” ì¶”ê°€)

**ë³€ê²½ ì „:**
```python
        elif config.user_encoder == 'OMAP':
            self.user_encoder = userEncoders.OMAP(self.news_encoder, config)
        # For ablations
```

**ë³€ê²½ í›„:**
```python
        elif config.user_encoder == 'OMAP':
            self.user_encoder = userEncoders.OMAP(self.news_encoder, config)
        elif config.user_encoder == 'MINER':
            self.user_encoder = userEncoders.MINER(self.news_encoder, config)
        # For ablations
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- MINER ìœ ì € ì¸ì½”ë”ë¥¼ ëª¨ë¸ ì„ íƒ ë¶„ê¸°ì— ë“±ë¡

---

**ìœ„ì¹˜**: Line 134 (Forward ì‹œê·¸ë‹ˆì²˜ ë³€ê²½)

**ë³€ê²½ ì „:**
```python
user_representation = self.user_encoder(..., news_representation)
```

**ë³€ê²½ í›„:**
```python
user_representation = self.user_encoder(..., news_representation, news_category)
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- MINERì˜ **category-aware attention**ì„ ìœ„í•´ í›„ë³´ ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬ ì •ë³´ (`news_category`) ì „ë‹¬
- ì¹´í…Œê³ ë¦¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— í•„ìš” (íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ì™€ í›„ë³´ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ê°„ ìœ ì‚¬ë„)

---

#### 2.5 newsEncoders.py

**ìœ„ì¹˜**: Line 42-115 (GloVe ì´ˆê¸°í™” ë©”ì„œë“œ ì¶”ê°€)

**ë³€ê²½ ì „:**
NewsEncoder í´ë˜ìŠ¤ì— í•´ë‹¹ ë©”ì„œë“œ ì—†ìŒ

**ë³€ê²½ í›„:**
```python
def load_category_embeddings_from_glove(self, category_dict, frozen=True):
    """GloVe 840B 300dë¡œ category embedding ì´ˆê¸°í™”"""
    glove = GloVe(name='840B', dim=300, cache='/home/user/jaesung/newsreclib/data/glove')

    category_emb_dim = self.category_embedding.weight.size(1)  # 50

    for category_name, idx in category_dict.items():
        # ë³µí•©ì–´ ì²˜ë¦¬: 'foodanddrink' â†’ ['food', 'drink']
        words = preprocess_category(category_name)

        # GloVe ë²¡í„° ìˆ˜ì§‘ ë° í‰ê· 
        vectors = [glove.vectors[glove.stoi[word]] for word in words if word in glove.stoi]
        if len(vectors) > 0:
            avg_vector = torch.stack(vectors).mean(dim=0)  # [300]

            # Dimension ì¡°ì • (300 â†’ 50: Truncate)
            category_vector = avg_vector[:category_emb_dim]
            self.category_embedding.weight.data[idx] = category_vector

    # Frozen ì„¤ì •
    self.category_embedding.weight.requires_grad = not frozen
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **GloVe 840B 300d í™œìš©**: ì¹´í…Œê³ ë¦¬ ì´ë¦„ì˜ ì˜ë¯¸ì  í‘œí˜„ì„ ì‚¬ì „í•™ìŠµ ë²¡í„°ë¡œ ì´ˆê¸°í™”
- **ë³µí•©ì–´ ì²˜ë¦¬**: 'foodanddrink' â†’ ['food', 'drink']ë¡œ ë¶„ë¦¬ í›„ í‰ê·  ë²¡í„° ê³„ì‚°
- **ì°¨ì› ì¡°ì •**: GloVe 300ì°¨ì› â†’ Category embedding 50ì°¨ì›ìœ¼ë¡œ Truncate
- **Frozen ì˜µì…˜**: `frozen=True`ì‹œ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© í•™ìŠµ ë¹„í™œì„±í™” (ì˜ë¯¸ì  í‘œí˜„ ìœ ì§€)
- **ëª©ì **: MINERì˜ category-aware attentionì—ì„œ ì˜ë¯¸ ìˆëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥

---

**ìœ„ì¹˜**: Line 513-534 (PLMNewsEncoder ë²„ê·¸ ìˆ˜ì •)

**ë³€ê²½ ì „:**
```python
class PLMNewsEncoder(NewsEncoder):
    def __init__(self, config: Config):
        super(PLMNewsEncoder, self).__init__()  # âŒ config ëˆ„ë½

        if self.pooling_method == 'attention':
            self.attention = Attention(hidden_dim=self.plm_hidden_dim, ...)  # âŒ ì˜ëª»ëœ íŒŒë¼ë¯¸í„°ëª…
```

**ë³€ê²½ í›„:**
```python
class PLMNewsEncoder(NewsEncoder):
    def __init__(self, config: Config):
        super(PLMNewsEncoder, self).__init__(config)  # âœ… config ì „ë‹¬

        if self.pooling_method == 'attention':
            self.attention = Attention(feature_dim=self.plm_hidden_dim, ...)  # âœ… ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°ëª…
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **`super().__init__(config)` ëˆ„ë½ ë²„ê·¸ ìˆ˜ì •**: NewsEncoder ì´ˆê¸°í™” ì‹œ config ì „ë‹¬ ëˆ„ë½ìœ¼ë¡œ ì¸í•œ ì—ëŸ¬ ë°©ì§€
- **Attention íŒŒë¼ë¯¸í„°ëª… ìˆ˜ì •**: `hidden_dim` â†’ `feature_dim`ìœ¼ë¡œ Attention í´ë˜ìŠ¤ ì¸í„°í˜ì´ìŠ¤ì— ë§ê²Œ ìˆ˜ì •

---

**ìœ„ì¹˜**: Line 703-784 (PLMMiner í´ë˜ìŠ¤ ì¶”ê°€)

**ë³€ê²½ ì „:**
íŒŒì¼ ë

**ë³€ê²½ í›„:**
```python
class PLMMiner(PLMNewsEncoder):
    """PLM + GloVe category embedding for MINER"""

    def __init__(self, config: Config, category_dict: dict):
        super(PLMMiner, self).__init__(config)

        # Category embeddings (MINERì˜ category-aware attentionìš©)
        self.category_embedding = nn.Embedding(
            num_embeddings=config.category_num,  # 17
            embedding_dim=config.category_embedding_dim  # 50
        )

        self.news_embedding_dim = self.plm_hidden_dim  # 768
        self.category_dict = category_dict
        self.use_category_glove = config.use_category_glove

    def initialize(self):
        super().initialize()

        # Random ì´ˆê¸°í™”
        nn.init.uniform_(self.category_embedding.weight, -0.1, 0.1)

        # GloVe ì´ˆê¸°í™” (if enabled)
        if self.use_category_glove:
            self.load_category_embeddings_from_glove(
                category_dict=self.category_dict,
                frozen=True  # ì¹´í…Œê³ ë¦¬ ì„ë² ë”© í•™ìŠµ ë¹„í™œì„±í™”
            )

    def forward(self, title_text, title_mask, ...):
        """
        Args:
            title_text: [B, N, 32] - PLM token IDs
            title_mask: [B, N, 32] - PLM attention mask

        Returns:
            news_repr: [B, N, 768] - PLM ì¶œë ¥ (category/subcategory fusion ì—†ìŒ)
        """
        B, N, L = title_text.size()

        # Reshape: [B, N, 32] â†’ [B*N, 32]
        title_text = title_text.view(B*N, L)
        title_mask = title_mask.view(B*N, L)

        # PLM encoding
        plm_output = self.plm(input_ids=title_text, attention_mask=title_mask)
        hidden_states = plm_output.last_hidden_state  # [B*N, 32, 768]

        # Pooling (attention-based)
        news_repr = self._pool_hidden_states(hidden_states, title_mask)  # [B*N, 768]
        news_repr = self.dropout(news_repr)

        # Reshape back: [B*N, 768] â†’ [B, N, 768]
        news_repr = news_repr.view(B, N, 768)

        return news_repr
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **PLMNAMLê³¼ì˜ ì°¨ì´ì **:
  - PLMNAML: PLM ì¶œë ¥ + Category/SubCategory fusion â†’ `[B, N, 868]` (768+50+50)
  - PLMMiner: PLM ì¶œë ¥ë§Œ ë°˜í™˜ â†’ `[B, N, 768]`
- **Category embedding ë³„ë„ ê´€ë¦¬**: MINERê°€ ì§ì ‘ category embeddingì— ì ‘ê·¼í•˜ì—¬ category-aware attention ìˆ˜í–‰
- **GloVe ìë™ ì´ˆê¸°í™”**: `--use_category_glove` í”Œë˜ê·¸ ì‹œ `initialize()`ì—ì„œ ìë™ìœ¼ë¡œ GloVe ì´ˆê¸°í™”
- **Frozen embedding**: ì¹´í…Œê³ ë¦¬ ì„ë² ë”© í•™ìŠµ ë¹„í™œì„±í™”ë¡œ GloVeì˜ ì˜ë¯¸ì  í‘œí˜„ ë³´ì¡´

---

#### 2.6 trainer.py

**ìœ„ì¹˜**: Line 9-36 (LR Scheduler ì¶”ê°€)

**ë³€ê²½ ì „:**
```python
from torch.nn.parallel import DistributedDataParallel as DDP

class Trainer:
    def __init__(self, model, config, mind_corpus, run_index):
        self.optimizer = optim.Adam(...)
        self.train_dataset = MIND_Train_Dataset(mind_corpus)
```

**ë³€ê²½ í›„:**
```python
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import get_linear_schedule_with_warmup

class Trainer:
    def __init__(self, model, config, mind_corpus, run_index):
        self.optimizer = optim.Adam(...)
        self.train_dataset = MIND_Train_Dataset(mind_corpus)

        # LR Scheduler ì¶”ê°€ (10% warmup + linear decay)
        total_steps = len(self.train_dataset) // config.batch_size * config.epoch
        warmup_steps = int(total_steps * 0.1)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **Learning Rate Scheduling**: ì´ˆê¸° 10% step ë™ì•ˆ linear warmup â†’ ì´í›„ linear decay
- **PLM Fine-tuning ì•ˆì •í™”**: Warmupìœ¼ë¡œ í° gradientì— ì˜í•œ ì‚¬ì „í•™ìŠµ ì§€ì‹ ì†ìƒ ë°©ì§€
- **Step ë‹¨ìœ„ ì—…ë°ì´íŠ¸**: `scheduler.step()`ì„ ë§¤ ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œ

---

**ìœ„ì¹˜**: Line 86-141 (Progress Bar ê°œì„ )

**ë³€ê²½ ì „:**
```python
for e in tqdm(range(1, self.epoch + 1)):
    for (...) in train_dataloader:
        ...
        self.optimizer.step()
```

**ë³€ê²½ í›„:**
```python
for e in tqdm(range(1, self.epoch + 1), desc='Epoch'):
    train_dataloader_with_progress = tqdm(train_dataloader, desc=f'Epoch {e}/{self.epoch}', leave=False)

    for (...) in train_dataloader_with_progress:
        ...
        epoch_loss += float(loss) * user_ID.size(0)

        # ì‹¤ì‹œê°„ loss í‘œì‹œ
        train_dataloader_with_progress.set_postfix({
            'loss': f'{float(loss):.4f}',
            'avg_loss': f'{epoch_loss / ((train_dataloader_with_progress.n + 1) * self.batch_size):.4f}'
        })

        self.optimizer.step()
        self.scheduler.step()  # LR scheduler ì—…ë°ì´íŠ¸
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **2ë‹¨ê³„ Progress Bar**: Epoch ë ˆë²¨ + Batch ë ˆë²¨ ì§„í–‰ë¥  í‘œì‹œ
- **ì‹¤ì‹œê°„ Loss ëª¨ë‹ˆí„°ë§**: í˜„ì¬ ë°°ì¹˜ loss + ëˆ„ì  í‰ê·  loss í‘œì‹œë¡œ í•™ìŠµ ì•ˆì •ì„± ì¦‰ì‹œ í™•ì¸
- **Scheduler í†µí•©**: ë§¤ ë°°ì¹˜ë§ˆë‹¤ `scheduler.step()` í˜¸ì¶œ

---

#### 2.7 userEncoders.py

**ìœ„ì¹˜**: Line 9-10 (UserEncoder ì‹œê·¸ë‹ˆì²˜ ë³€ê²½)

**ë³€ê²½ ì „:**
```python
def forward(self, ..., candidate_news_representation):
```

**ë³€ê²½ í›„:**
```antml:parameter>
def forward(self, ..., candidate_news_representation, candidate_category=None):
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- `candidate_category` íŒŒë¼ë¯¸í„° ì¶”ê°€ë¡œ MINERì˜ category-aware attention ì§€ì›
- ê¸°ë³¸ê°’ `None`ìœ¼ë¡œ backward compatibility ìœ ì§€ (ê¸°ì¡´ ì¸ì½”ë”ëŠ” ë¬´ì‹œ)

---

**ìœ„ì¹˜**: Line 375-631 (MINER í´ë˜ìŠ¤ ì¶”ê°€)

**ë³€ê²½ ì „:**
íŒŒì¼ ë

**ë³€ê²½ í›„:**
```python
class MINER(UserEncoder):
    """
    Multi-Interest News Encoder with Poly Attention
    - Poly Attention: Kê°œì˜ context codesë¡œ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ ì¶”ì¶œ
    - Disagreement Regularization: Interest vector ë‹¤ì–‘ì„± ìœ ë„
    - Category-Aware Attention: ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ ê¸°ë°˜ attention ê°€ì¤‘ì¹˜ ì¡°ì •
    - Score Aggregation: max/mean/weighted ì§‘ê³„
    """

    def __init__(self, news_encoder, config):
        super(MINER, self).__init__(news_encoder, config)

        # Poly attention parameters
        self.K = config.num_interest_vectors  # 32
        self.context_dim = config.context_code_dim  # 200
        self.aggregation = config.miner_aggregation  # 'weighted'
        self.disagreement_beta = config.disagreement_beta  # 0.8

        # Kê°œì˜ learnable context codes [32, 200]
        self.context_codes = nn.Parameter(torch.zeros(self.K, self.context_dim))

        # Projection layer: [768] â†’ [200]
        self.W_h = nn.Linear(self.news_embedding_dim, self.context_dim, bias=False)

        # Target-aware attention (weighted aggregation)
        if self.aggregation == 'weighted':
            self.W_e = nn.Linear(self.news_embedding_dim, self.news_embedding_dim, bias=True)

        self.dropout = nn.Dropout(p=config.dropout_rate)
        self.attention_scalar = math.sqrt(float(self.context_dim))  # âˆš200

        # Category-aware attention
        self.category_aware_lambda = nn.Parameter(torch.tensor(config.category_aware_lambda))  # 0.5 (learnable)
        self.category_embedding = news_encoder.category_embedding  # PLMMinerì˜ category embedding ì°¸ì¡°

    def initialize(self):
        nn.init.orthogonal_(self.context_codes.data)  # Context codes: orthogonal
        nn.init.xavier_uniform_(self.W_h.weight)  # Projection: Xavier
        if self.aggregation == 'weighted':
            nn.init.xavier_uniform_(self.W_e.weight)
            nn.init.zeros_(self.W_e.bias)
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **Context Codes `[32, 200]`**: Orthogonal ì´ˆê¸°í™”ë¡œ ì„œë¡œ ì§êµí•˜ëŠ” 32ê°œì˜ ì¿¼ë¦¬ ë²¡í„° ìƒì„± (ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ í‘œí˜„)
- **Projection Layer `W_h`**: ë‰´ìŠ¤ ì„ë² ë”© 768ì°¨ì› â†’ Context ì°¨ì› 200ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
- **Target-Aware Attention `W_e`**: í›„ë³´ ë‰´ìŠ¤ë¥¼ ê³ ë ¤í•œ ê´€ì‹¬ì‚¬ ê°€ì¤‘ ì§‘ê³„ (weighted aggregation ì‹œ)
- **Category-Aware Lambda**: Learnable íŒŒë¼ë¯¸í„°ë¡œ ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¡°ì • (ì´ˆê¸°ê°’ 0.5)
- **Category Embedding ì°¸ì¡°**: PLMMinerì˜ GloVe ì´ˆê¸°í™”ëœ category embedding ê³µìœ 

---

**MINER Forward Pass êµ¬ì¡°:**

```python
def forward(self, ..., candidate_news_representation, candidate_category=None):
    """
    Args:
        user_title_text: [B, M, 32] - íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ PLM token IDs
        user_category: [B, M] - íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ IDs
        user_history_mask: [B, M] - íˆìŠ¤í† ë¦¬ ë§ˆìŠ¤í¬
        candidate_news_representation: [B, N, 768] - í›„ë³´ ë‰´ìŠ¤ PLM ì¶œë ¥
        candidate_category: [B, N] - í›„ë³´ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ IDs

    Returns:
        user_representation: [B, N, 768]
    """
    B = user_title_text.size(0)
    N = candidate_news_representation.size(1)

    # 1. íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ ì¸ì½”ë”© [B, M, 768]
    history_embedding = self.news_encoder(user_title_text, ...)

    # 2. Poly Attention: Kê°œì˜ interest vectors ì¶”ì¶œ
    interest_vectors = self.poly_attention(
        history_embedding,  # [B, M, 768]
        user_history_mask,  # [B, M]
        user_category,  # [B, M]
        candidate_category  # [B, N]
    )  # â†’ [B, N, K, 768] (category-aware) or [B, K, 768]

    # 3. Disagreement Regularization (training ì‹œ)
    if self.training:
        self.auxiliary_loss = self.disagreement_beta * self.compute_disagreement_loss(interest_vectors)

    # 4. Score Aggregation
    if self.aggregation == 'weighted':
        # Target-aware weighted sum
        W_e_h_c = F.gelu(self.W_e(candidate_news_representation))  # [B, N, 768]
        logits = torch.matmul(W_e_h_c.unsqueeze(2), interest_vectors.transpose(2, 3)).squeeze(2)  # [B, N, K]
        alpha = F.softmax(logits, dim=2)  # [B, N, K]
        user_representation = (alpha.unsqueeze(3) * interest_vectors).sum(dim=2)  # [B, N, 768]

    return user_representation
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **Poly Attention**: íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ë¡œë¶€í„° K=32ê°œì˜ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ ë²¡í„° ì¶”ì¶œ
- **Category-Aware Weighting**: íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ì™€ í›„ë³´ ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ attentionì— ë°˜ì˜
- **Disagreement Loss**: Interest vector ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì†Œí™”ë¡œ ë‹¤ì–‘ì„± ê°•ì œ (auxiliary loss)
- **Weighted Aggregation**: í›„ë³´ ë‰´ìŠ¤ë³„ë¡œ ê°€ì¥ ê´€ë ¨ ë†’ì€ interest vectorsì— ì§‘ì¤‘

---

**Poly Attention ì„¸ë¶€ êµ¬ì¡°:**

```python
def poly_attention(self, history_embeddings, history_mask, user_category=None, candidate_category=None):
    """
    Args:
        history_embeddings: [B, M, 768]
        history_mask: [B, M]
        user_category: [B, M] - íˆìŠ¤í† ë¦¬ ì¹´í…Œê³ ë¦¬
        candidate_category: [B, N] - í›„ë³´ ì¹´í…Œê³ ë¦¬

    Returns:
        interest_vectors: [B, N, K, 768] (category-aware) or [B, K, 768]
    """
    B, M, D = history_embeddings.size()

    # Project history embeddings: [B, M, 768] â†’ [B, M, 200]
    h_proj = torch.tanh(self.W_h(history_embeddings))

    # Attention logits: [B, M, 200] @ [200, 32] = [B, M, 32]
    logits = torch.matmul(h_proj, self.context_codes.T) / self.attention_scalar

    if user_category is not None and candidate_category is not None:
        # Category-aware attention
        hist_cat_emb = self.category_embedding(user_category)  # [B, M, 50]
        cand_cat_emb = self.category_embedding(candidate_category)  # [B, N, 50]

        # Cosine similarity: [B, M, 50] @ [B, 50, N] = [B, M, N]
        hist_cat_norm = F.normalize(hist_cat_emb, p=2, dim=2)
        cand_cat_norm = F.normalize(cand_cat_emb, p=2, dim=2)
        category_sim = torch.bmm(hist_cat_norm, cand_cat_norm.transpose(1, 2))

        # Expand logits: [B, M, 32] â†’ [B, M, N, 32]
        N = candidate_category.size(1)
        logits_expanded = logits.unsqueeze(2).expand(-1, -1, N, -1)

        # Add category bias: logits + Î» * cos(category)
        category_bias = self.category_aware_lambda * category_sim.unsqueeze(3)  # [B, M, N, 1]
        logits = logits_expanded + category_bias  # [B, M, N, 32]

        # Mask and softmax
        mask_expanded = history_mask.unsqueeze(2).unsqueeze(3).expand(-1, -1, N, self.K)
        logits = logits.masked_fill(mask_expanded == 0, -1e9)
        attn_weights = F.softmax(logits, dim=1)  # [B, M, N, 32]

        # Weighted sum: [B, N, 32, M] @ [B, M, 768] = [B, N, 32, 768]
        interest_vectors = torch.einsum('bmnk,bmd->bnkd', attn_weights, history_embeddings)
    else:
        # Category-agnostic poly attention
        mask_expanded = history_mask.unsqueeze(2).expand(-1, -1, self.K)
        logits = logits.masked_fill(mask_expanded == 0, -1e9)
        attn_weights = F.softmax(logits, dim=1)  # [B, M, 32]
        interest_vectors = torch.bmm(attn_weights.transpose(1, 2), history_embeddings)  # [B, 32, 768]

    return interest_vectors
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **Additive Attention**: `tanh(W_h @ h_j)` projection í›„ context codesì™€ ë‚´ì 
- **Category-Aware Weighting**:
  - íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ì™€ í›„ë³´ ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
  - `Î» * cos(cat_history, cat_candidate)`ë¥¼ attention logitsì— ê°€ì‚°
  - ê°™ì€ ì¹´í…Œê³ ë¦¬ íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ì— ë” ë†’ì€ attention ë¶€ì—¬
- **ì°¨ì› í™•ì¥**: [B, M, K] â†’ [B, M, N, K]ë¡œ í™•ì¥í•˜ì—¬ í›„ë³´ ë‰´ìŠ¤ë³„ ê´€ì‹¬ì‚¬ ë²¡í„° ìƒì„±
- **GloVe ì˜ì¡´ì„±**: `category_embedding`ì´ GloVeë¡œ ì´ˆê¸°í™”ë˜ì–´ì•¼ ì˜ë¯¸ ìˆëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥

---

**Disagreement Regularization:**

```python
def compute_disagreement_loss(self, interest_vectors):
    """
    Args:
        interest_vectors: [B, K, 768] or [B, N, K, 768]

    Returns:
        loss: scalar (í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    """
    # Reshape if needed
    if interest_vectors.dim() == 4:
        B, N, K, D = interest_vectors.size()
        interest_vectors = interest_vectors.view(B*N, K, D)

    # Normalize: [B, K, 768]
    normalized = F.normalize(interest_vectors, p=2, dim=2)

    # Pairwise cosine similarity: [B, K, 768] @ [B, 768, K] = [B, K, K]
    similarity_matrix = torch.bmm(normalized, normalized.transpose(1, 2))

    # Average over all pairs
    loss = similarity_matrix.sum(dim=(1, 2)) / (K * K)

    return loss.mean()
```

**ğŸ’¡ í•µì‹¬ ìš”ì•½:**
- **ëª©ì **: Interest vectors ê°„ ë‹¤ì–‘ì„± ìœ ë„ (ì„œë¡œ ë‹¤ë¥¸ ê´€ì‹¬ì‚¬ í‘œí˜„)
- **ë°©ë²•**: KÃ—K pairwise cosine similarity í‰ê· ì„ ìµœì†Œí™”
- **íš¨ê³¼**: Context codesê°€ orthogonal ì´ˆê¸°í™”ë˜ì—ˆì§€ë§Œ, í•™ìŠµ ê³¼ì •ì—ì„œ collapse ë°©ì§€
- **Loss í†µí•©**: `total_loss = click_loss + Î² * disagreement_loss` (Î²=0.8)

---

### ë³€ê²½ ì‚¬í•­ ì „ì²´ ìš”ì•½

| íŒŒì¼ | í•µì‹¬ ë³€ê²½ | ëª©ì  |
|------|----------|------|
| **MIND_corpus.py** | PLM ë°ì´í„°ë¥¼ ê¸°ì¡´ í•„ë“œì— ì˜¤ë²„ë¼ì´ë“œ | ë°ì´í„° ë¡œë” í˜¸í™˜ì„± ìœ ì§€ |
| **config.py** | MINER íŒŒë¼ë¯¸í„° 6ê°œ ì¶”ê°€ + PLM ìë™ ì„¤ì • | MINER í•˜ì´í¼íŒŒë¼ë¯¸í„° ì§€ì› |
| **main.py** | `category_dict` ì „ë‹¬ | GloVe ì´ˆê¸°í™”ìš© ë©”íƒ€ë°ì´í„° ì œê³µ |
| **model.py** | PLMMiner/MINER ë“±ë¡ + `news_category` ì „ë‹¬ | ìƒˆ ì¸ì½”ë” ì§€ì› + category-aware attention |
| **newsEncoders.py** | GloVe ì´ˆê¸°í™” ë©”ì„œë“œ + PLMMiner í´ë˜ìŠ¤ | ì˜ë¯¸ì  category embedding |
| **trainer.py** | LR Scheduler + Progress Bar | PLM fine-tuning ì•ˆì •í™” + ëª¨ë‹ˆí„°ë§ |
| **userEncoders.py** | MINER í´ë˜ìŠ¤ (260ì¤„) | Poly attention + Category-aware attention |

---

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜

### 3.1 ì „ì²´ êµ¬ì¡°

```
Input (User Behavior)
  â”œâ”€ User History News: [B, M, 32] PLM token IDs
  â”œâ”€ User History Category: [B, M]
  â”œâ”€ Candidate News: [B, N, 32] PLM token IDs
  â””â”€ Candidate Category: [B, N]

    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLMMiner News Encoder                  â”‚
â”‚  Input: [B, M+N, 32] token IDs          â”‚
â”‚  â”œâ”€ BERT: [B, M+N, 32, 768]             â”‚
â”‚  â”œâ”€ Attention Pooling: [B, M+N, 768]    â”‚
â”‚  â””â”€ Output: [B, M+N, 768]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â†“ Split

History Embedding [B, M, 768]    Candidate Embedding [B, N, 768]

    â†“                                 â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MINER User Encoder                     â”‚
â”‚                                         â”‚
â”‚  1. Poly Attention                      â”‚
â”‚     â”œâ”€ Project: W_h @ h_j â†’ [B, M, 200]â”‚
â”‚     â”œâ”€ Attention: logits = h_proj @ c_k â”‚
â”‚     â”œâ”€ Category-Aware: + Î»*cos(cat)    â”‚
â”‚     â””â”€ Output: [B, N, K, 768]          â”‚
â”‚                                         â”‚
â”‚  2. Disagreement Loss                   â”‚
â”‚     â””â”€ Minimize cos(e_i, e_j)          â”‚
â”‚                                         â”‚
â”‚  3. Weighted Aggregation                â”‚
â”‚     â”œâ”€ W_e(h_c) â†’ [B, N, 768]          â”‚
â”‚     â”œâ”€ Attention: softmax(W_e @ E_k)   â”‚
â”‚     â””â”€ Output: [B, N, 768]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â†“

User Representation [B, N, 768]

    â†“

Dot Product: user_repr Â· candidate_repr â†’ [B, N]

    â†“

Softmax â†’ Click Prediction
```

---

### 3.2 PLMMiner News Encoder

#### ì…ë ¥/ì¶œë ¥ ì°¨ì› ë³€í™”

| Step | Operation | Input Shape | Output Shape |
|------|-----------|-------------|--------------|
| **Input** | PLM token IDs | `[B, N, 32]` | - |
| **Reshape** | `view(B*N, 32)` | `[B, N, 32]` | `[B*N, 32]` |
| **BERT** | `self.plm(input_ids, mask)` | `[B*N, 32]` | `[B*N, 32, 768]` |
| **Pooling** | Attention-based | `[B*N, 32, 768]` | `[B*N, 768]` |
| **Dropout** | - | `[B*N, 768]` | `[B*N, 768]` |
| **Reshape** | `view(B, N, 768)` | `[B*N, 768]` | `[B, N, 768]` |
| **Output** | News representation | - | `[B, N, 768]` |

**B**: Batch size (ì˜ˆ: 64)
**N**: News ê°œìˆ˜ (í›ˆë ¨: 1 positive + 4 negative = 5, í‰ê°€: ê°€ë³€)
**M**: History ê°œìˆ˜ (max 50)
**K**: Interest vectors ê°œìˆ˜ (32)

---

#### Attention Pooling ìƒì„¸

```python
def _pool_hidden_states(self, hidden_states, attention_mask):
    """
    Args:
        hidden_states: [B*N, 32, 768] - BERT ì¶œë ¥
        attention_mask: [B*N, 32] - Padding mask

    Returns:
        pooled: [B*N, 768]
    """
    if self.pooling_method == 'cls':
        # [CLS] í† í° ì‚¬ìš©
        pooled = hidden_states[:, 0, :]  # [B*N, 768]

    elif self.pooling_method == 'average':
        # Masked average pooling
        mask_expanded = attention_mask.unsqueeze(2).expand_as(hidden_states)  # [B*N, 32, 768]
        sum_hidden = (hidden_states * mask_expanded).sum(dim=1)  # [B*N, 768]
        sum_mask = mask_expanded.sum(dim=1)  # [B*N, 768]
        pooled = sum_hidden / sum_mask.clamp(min=1e-9)  # [B*N, 768]

    elif self.pooling_method == 'attention':
        # Attention-based pooling
        attn_weights = self.attention(hidden_states, attention_mask)  # [B*N, 32, 1]
        pooled = (attn_weights * hidden_states).sum(dim=1)  # [B*N, 768]

    return pooled
```

**Attention-based Pooling êµ¬ì¡°:**
```
hidden_states [B*N, 32, 768]
  â†“
tanh(W @ h_i) [B*N, 32, 200]
  â†“
v^T @ tanh(...) [B*N, 32, 1]
  â†“
softmax (masked) [B*N, 32, 1]
  â†“
weighted sum â†’ [B*N, 768]
```

**ğŸ’¡ í•µì‹¬:**
- **Attention í•™ìŠµ**: ê° í† í°ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ (ì œëª©ì˜ í•µì‹¬ ë‹¨ì–´ì— ì§‘ì¤‘)
- **Masking**: Padding í† í°ì€ softmax ì „ì— `-1e9`ë¡œ ë§ˆìŠ¤í‚¹
- **ì°¨ì› ìœ ì§€**: í† í° ì‹œí€€ìŠ¤ `[32, 768]` â†’ ë‹¨ì¼ ë²¡í„° `[768]`

---

#### Category Embedding (GloVe ì´ˆê¸°í™”)

```python
# ì´ˆê¸°í™” ì‹œ
self.category_embedding = nn.Embedding(17, 50)  # [category_num, category_emb_dim]

# GloVe ì´ˆê¸°í™” (if --use_category_glove)
for category_name, idx in category_dict.items():
    # 'sports' â†’ glove['sports'] [300]
    # 'foodanddrink' â†’ (glove['food'] + glove['drink']) / 2 [300]
    glove_vector = get_glove_vector(category_name)  # [300]
    truncated_vector = glove_vector[:50]  # [50]
    self.category_embedding.weight.data[idx] = truncated_vector

self.category_embedding.weight.requires_grad = False  # Frozen
```

**GloVe ì´ˆê¸°í™” íš¨ê³¼:**
```
Random ì´ˆê¸°í™”:
  cos(sports, entertainment) = -0.03 (ë¬´ì˜ë¯¸)
  cos(sports, finance) = 0.12 (ë¬´ì˜ë¯¸)

GloVe ì´ˆê¸°í™”:
  cos(sports, entertainment) = 0.42 (ê´€ë ¨ì„± ë°˜ì˜)
  cos(sports, finance) = 0.15 (ë‚®ì€ ê´€ë ¨ì„±)
```

**ğŸ’¡ í•µì‹¬:**
- **ì˜ë¯¸ì  í‘œí˜„**: ì¹´í…Œê³ ë¦¬ ê°„ ì‹¤ì œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ë°˜ì˜
- **MINER Category-Aware Attention**: ì˜ë¯¸ ìˆëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
- **Frozen**: í•™ìŠµ ì¤‘ GloVeì˜ ì˜ë¯¸ì  í‘œí˜„ ë³´ì¡´

---

### 3.3 MINER User Encoder

#### ì „ì²´ Forward Pass ì°¨ì› ë³€í™”

| Step | Operation | Input Shape | Output Shape |
|------|-----------|-------------|--------------|
| **1. History Encoding** | PLMMiner | `[B, M, 32]` | `[B, M, 768]` |
| **2. Poly Attention** | - | - | - |
|   2-1. Projection | `W_h @ h_j` | `[B, M, 768]` | `[B, M, 200]` |
|   2-2. Logits | `h_proj @ c_k` | `[B, M, 200]` Ã— `[32, 200]^T` | `[B, M, 32]` |
|   2-3. Category Bias | `Î» * cos(cat)` | - | `[B, M, N, 1]` |
|   2-4. Expand Logits | - | `[B, M, 32]` | `[B, M, N, 32]` |
|   2-5. Attention | `softmax(logits + bias)` | `[B, M, N, 32]` | `[B, M, N, 32]` |
|   2-6. Weighted Sum | `einsum` | attn `[B, M, N, 32]` Ã— h `[B, M, 768]` | `[B, N, 32, 768]` |
| **3. Disagreement Loss** | `cos(e_i, e_j)` | `[B, N, 32, 768]` | `scalar` |
| **4. Aggregation** | - | - | - |
|   4-1. W_e Transform | `W_e(h_c)` | `[B, N, 768]` | `[B, N, 768]` |
|   4-2. Attention | `W_e @ E_k` | `[B, N, 768]` Ã— `[B, N, 768, 32]` | `[B, N, 32]` |
|   4-3. Softmax | - | `[B, N, 32]` | `[B, N, 32]` |
|   4-4. Weighted Sum | `Î± * E_k` | Î± `[B, N, 32]` Ã— E `[B, N, 32, 768]` | `[B, N, 768]` |
| **Output** | User representation | - | `[B, N, 768]` |

---

#### Poly Attention ìƒì„¸

**Step 1: Projection**
```python
h_proj = torch.tanh(self.W_h(history_embeddings))
# Input:  [B, M, 768]
# W_h:    [768, 200]
# Output: [B, M, 200]
```

**Step 2: Attention Logits**
```python
logits = torch.matmul(h_proj, self.context_codes.T) / sqrt(200)
# h_proj:       [B, M, 200]
# context_codes: [32, 200]
# matmul:       [B, M, 200] @ [200, 32] = [B, M, 32]
# scaling:      / 14.14
# Output:       [B, M, 32]
```

**Step 3: Category-Aware Weighting**
```python
# íˆìŠ¤í† ë¦¬ ì¹´í…Œê³ ë¦¬ ì„ë² ë”©
hist_cat_emb = self.category_embedding(user_category)  # [B, M] â†’ [B, M, 50]
hist_cat_norm = F.normalize(hist_cat_emb, p=2, dim=2)  # [B, M, 50]

# í›„ë³´ ì¹´í…Œê³ ë¦¬ ì„ë² ë”©
cand_cat_emb = self.category_embedding(candidate_category)  # [B, N] â†’ [B, N, 50]
cand_cat_norm = F.normalize(cand_cat_emb, p=2, dim=2)  # [B, N, 50]

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„
category_sim = torch.bmm(hist_cat_norm, cand_cat_norm.transpose(1, 2))
# [B, M, 50] @ [B, 50, N] = [B, M, N]

# Logits í™•ì¥
logits_expanded = logits.unsqueeze(2).expand(-1, -1, N, -1)
# [B, M, 32] â†’ [B, M, 1, 32] â†’ [B, M, N, 32]

# Category bias ì¶”ê°€
category_bias = self.category_aware_lambda * category_sim.unsqueeze(3)
# [B, M, N] â†’ [B, M, N, 1]

logits_final = logits_expanded + category_bias
# [B, M, N, 32]
```

**ì˜ˆì‹œ:**
```
íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ j: 'Lakers win championship' (category: sports)
í›„ë³´ ë‰´ìŠ¤ c1: 'Football game today' (category: sports)
í›„ë³´ ë‰´ìŠ¤ c2: 'Stock market crash' (category: finance)

cos(sports, sports) = 0.9  â†’  bias = 0.5 * 0.9 = 0.45
cos(sports, finance) = 0.2  â†’  bias = 0.5 * 0.2 = 0.1

logits[j, c1, k] += 0.45  (ê°™ì€ ì¹´í…Œê³ ë¦¬ â†’ ë†’ì€ attention)
logits[j, c2, k] += 0.1   (ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ â†’ ë‚®ì€ attention)
```

**ğŸ’¡ í•µì‹¬:**
- **Category-aware**: í›„ë³´ ë‰´ìŠ¤ì™€ ì¹´í…Œê³ ë¦¬ê°€ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ì— ë” ë†’ì€ attention
- **Personalization**: ì‚¬ìš©ìì˜ ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„ ë°˜ì˜
- **GloVe ì˜ì¡´**: `category_embedding`ì´ ì˜ë¯¸ì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ì•¼ íš¨ê³¼ì 

---

**Step 4: Attention Weights & Weighted Sum**
```python
# Masking
mask_expanded = history_mask.unsqueeze(2).unsqueeze(3).expand(-1, -1, N, K)
# [B, M] â†’ [B, M, 1, 1] â†’ [B, M, N, 32]

logits_masked = logits_final.masked_fill(mask_expanded == 0, -1e9)
# Padding íˆìŠ¤í† ë¦¬ ì œê±°

# Softmax over history dimension
attn_weights = F.softmax(logits_masked, dim=1)
# [B, M, N, 32] â†’ softmax(dim=1) â†’ [B, M, N, 32]

# Weighted sum
interest_vectors = torch.einsum('bmnk,bmd->bnkd', attn_weights, history_embeddings)
# attn:   [B, M, N, 32]
# hist:   [B, M, 768]
# output: [B, N, 32, 768]
```

**Einsum ìƒì„¸:**
```
b: batch
m: history news
n: candidate news
k: interest vector index (32)
d: embedding dimension (768)

bmnk, bmd -> bnkd
= for each (b, n, k, d):
    output[b, n, k, d] = Î£_m attn[b, m, n, k] * hist[b, m, d]
```

**ğŸ’¡ í•µì‹¬:**
- **í›„ë³´ë³„ ê´€ì‹¬ì‚¬**: ê° í›„ë³´ ë‰´ìŠ¤ì— ëŒ€í•´ 32ê°œì˜ interest vectors ìƒì„±
- **Soft Selection**: Softmaxë¡œ íˆìŠ¤í† ë¦¬ ë‰´ìŠ¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°
- **ë‹¤ì–‘ì„±**: 32ê°œì˜ context codesê°€ ì„œë¡œ ë‹¤ë¥¸ aspect ìº¡ì²˜

---

#### Disagreement Regularization

```python
def compute_disagreement_loss(self, interest_vectors):
    """
    Args:
        interest_vectors: [B, N, 32, 768]

    Returns:
        loss: scalar
    """
    B, N, K, D = interest_vectors.size()

    # Reshape: [B*N, 32, 768]
    interest_vectors = interest_vectors.view(B*N, K, D)

    # Normalize
    normalized = F.normalize(interest_vectors, p=2, dim=2)  # [B*N, 32, 768]

    # Pairwise cosine similarity
    similarity_matrix = torch.bmm(normalized, normalized.transpose(1, 2))
    # [B*N, 32, 768] @ [B*N, 768, 32] = [B*N, 32, 32]

    # Average over all pairs
    K = interest_vectors.size(1)
    loss = similarity_matrix.sum(dim=(1, 2)) / (K * K)
    # [B*N, 32, 32] â†’ sum â†’ [B*N] â†’ mean â†’ scalar

    return loss.mean()
```

**Similarity Matrix ì˜ˆì‹œ:**
```
[e_1, e_2, ..., e_32]

similarity_matrix[i, j] = cos(e_i, e_j)

ì´ìƒì ì¸ ê²½ìš°:
  [[1.0, 0.0, 0.0, ..., 0.0],
   [0.0, 1.0, 0.0, ..., 0.0],
   ...
   [0.0, 0.0, 0.0, ..., 1.0]]

í‰ê·  = 32 / (32*32) = 0.03125 (ìµœì†Œ)

ì‹¤ì œ í•™ìŠµ ì´ˆê¸°:
  í‰ê·  ~ 0.2 (ë†’ì€ ìœ ì‚¬ë„)

í•™ìŠµ í›„:
  í‰ê·  ~ 0.05 (ë‚®ì€ ìœ ì‚¬ë„, ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬)
```

**Loss í†µí•©:**
```python
total_loss = click_prediction_loss + Î² * disagreement_loss
           = CrossEntropy(logits, labels) + 0.8 * disagreement_loss
```

**ğŸ’¡ í•µì‹¬:**
- **ë‹¤ì–‘ì„± ê°•ì œ**: Interest vectorsê°€ ì„œë¡œ ë…ë¦½ì ì¸ ê´€ì‹¬ì‚¬ë¥¼ í‘œí˜„í•˜ë„ë¡ ìœ ë„
- **Collapse ë°©ì§€**: ëª¨ë“  interest vectorsê°€ ë™ì¼í•œ í‘œí˜„ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒ ë°©ì§€
- **Regularization**: Î²=0.8ë¡œ click predictionê³¼ ê· í˜•

---

#### Weighted Aggregation

```python
# Step 1: Target-aware transformation
W_e_h_c = F.gelu(self.W_e(candidate_news_representation))
# candidate: [B, N, 768]
# W_e:       [768, 768] + bias
# output:    [B, N, 768]

# Step 2: Attention logits
logits = torch.matmul(
    W_e_h_c.unsqueeze(2),  # [B, N, 1, 768]
    interest_vectors.transpose(2, 3)  # [B, N, 768, 32]
)
# matmul: [B, N, 1, 768] @ [B, N, 768, 32] = [B, N, 1, 32]

logits = logits.squeeze(2)  # [B, N, 32]

# Step 3: Softmax over interest vectors
alpha = F.softmax(logits, dim=2)  # [B, N, 32]

# Step 4: Weighted sum
user_representation = (alpha.unsqueeze(3) * interest_vectors).sum(dim=2)
# alpha:     [B, N, 32, 1]
# interest:  [B, N, 32, 768]
# multiply:  [B, N, 32, 768]
# sum(dim=2): [B, N, 768]
```

**ì˜ˆì‹œ:**
```
í›„ë³´ ë‰´ìŠ¤ c: 'Lakers win championship'

32ê°œì˜ interest vectors:
  e_1: ìŠ¤í¬ì¸  ê´€ë ¨ (cos(W_e(h_c), e_1) = 0.8)
  e_2: ì—”í„°í…Œì¸ë¨¼íŠ¸ ê´€ë ¨ (cos = 0.3)
  e_3: ì •ì¹˜ ê´€ë ¨ (cos = 0.1)
  ...

Softmax í›„:
  Î±_1 = 0.6 (ë†’ì€ ê°€ì¤‘ì¹˜)
  Î±_2 = 0.2
  Î±_3 = 0.05
  ...

user_repr = 0.6 * e_1 + 0.2 * e_2 + 0.05 * e_3 + ...
```

**ğŸ’¡ í•µì‹¬:**
- **Target-aware**: í›„ë³´ ë‰´ìŠ¤ì™€ ê°€ì¥ ê´€ë ¨ ë†’ì€ interest vectorsì— ì§‘ì¤‘
- **Dynamic Weighting**: í›„ë³´ ë‰´ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¸ ê´€ì‹¬ì‚¬ ì¡°í•© ì‚¬ìš©
- **Personalization**: ì‚¬ìš©ìì˜ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ ì¤‘ í›„ë³´ì™€ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì„ íƒ

---

### 3.4 ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

#### ì…ë ¥ ë°ì´í„°

```python
Batch:
  User History (M=3):
    News 1: 'Lakers win NBA championship' (category: sports)
    News 2: 'New iPhone release' (category: tech)
    News 3: 'Movie review: Avengers' (category: entertainment)

  Candidate News (N=2):
    News A: 'Football game highlights' (category: sports)
    News B: 'Stock market update' (category: finance)

Shapes:
  user_title_text: [1, 3, 32]  (PLM token IDs)
  user_category: [1, 3]  (category IDs)
  candidate_news_representation: [1, 2, 768]  (PLM ì¶œë ¥)
  candidate_category: [1, 2]
```

---

#### Forward Pass

**1. History Encoding**
```
PLMMiner(user_title_text):
  Input:  [1, 3, 32]
  BERT:   [3, 32] â†’ [3, 32, 768]
  Pool:   [3, 32, 768] â†’ [3, 768]
  Output: [1, 3, 768]

history_embedding:
  h_1: [768-dim vector for 'Lakers...']
  h_2: [768-dim vector for 'iPhone...']
  h_3: [768-dim vector for 'Avengers...']
```

---

**2. Poly Attention**

```
Projection:
  h_proj_1 = tanh(W_h @ h_1): [200]
  h_proj_2 = tanh(W_h @ h_2): [200]
  h_proj_3 = tanh(W_h @ h_3): [200]

Attention Logits (before category bias):
  logits_1 = h_proj_1 @ c_k^T: [32]  (ì˜ˆ: [0.5, 0.3, 0.1, ...])
  logits_2 = h_proj_2 @ c_k^T: [32]
  logits_3 = h_proj_3 @ c_k^T: [32]

Category Similarity:
  cos(sports, sports) = 0.9
  cos(tech, sports) = 0.2
  cos(entertainment, sports) = 0.4
  cos(sports, finance) = 0.1
  cos(tech, finance) = 0.3
  cos(entertainment, finance) = 0.15

Category Bias (Î»=0.5):
  For News A (sports):
    bias_1A = 0.5 * 0.9 = 0.45
    bias_2A = 0.5 * 0.2 = 0.1
    bias_3A = 0.5 * 0.4 = 0.2

  For News B (finance):
    bias_1B = 0.5 * 0.1 = 0.05
    bias_2B = 0.5 * 0.3 = 0.15
    bias_3B = 0.5 * 0.15 = 0.075

Final Logits (example for k=0):
  logits[1, A, 0] = 0.5 + 0.45 = 0.95  (h_1ì— ë†’ì€ attention)
  logits[2, A, 0] = 0.3 + 0.1 = 0.4
  logits[3, A, 0] = 0.1 + 0.2 = 0.3

  logits[1, B, 0] = 0.5 + 0.05 = 0.55
  logits[2, B, 0] = 0.3 + 0.15 = 0.45
  logits[3, B, 0] = 0.1 + 0.075 = 0.175

Softmax (over history):
  For News A, k=0:
    Î±_1 = exp(0.95) / (exp(0.95) + exp(0.4) + exp(0.3)) = 0.6
    Î±_2 = 0.25
    Î±_3 = 0.15

  For News B, k=0:
    Î±_1 = 0.4
    Î±_2 = 0.35
    Î±_3 = 0.25

Interest Vectors:
  E_A[0] = 0.6 * h_1 + 0.25 * h_2 + 0.15 * h_3  (ìŠ¤í¬ì¸  ì¤‘ì‹¬)
  E_B[0] = 0.4 * h_1 + 0.35 * h_2 + 0.25 * h_3  (ê¸°ìˆ /ì¬ë¬´ í˜¼í•©)
  ... (k=1~31ë„ ë™ì¼í•˜ê²Œ)

Shape:
  interest_vectors: [1, 2, 32, 768]
```

**ğŸ’¡ í•µì‹¬:**
- News A (sports): h_1 (sports íˆìŠ¤í† ë¦¬)ì— ë†’ì€ attention (0.6)
- News B (finance): h_2 (tech)ì™€ h_1ì— ë¹„ìŠ·í•œ attention (ì¬ë¬´ì™€ ê´€ë ¨ ì—†ì§€ë§Œ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒ ì„ íƒ)

---

**3. Disagreement Loss**

```
Interest vectors for News A:
  E_A[0], E_A[1], ..., E_A[31]

Pairwise Cosine Similarity:
  cos(E_A[0], E_A[1]) = 0.12
  cos(E_A[0], E_A[2]) = 0.08
  ...

Average: 0.05

Total Loss:
  click_loss + 0.8 * 0.05
```

---

**4. Weighted Aggregation**

```
Target-aware transform:
  W_e_A = gelu(W_e(h_A)): [768]
  W_e_B = gelu(W_e(h_B)): [768]

Attention logits:
  For News A:
    logits_A[k] = W_e_A Â· E_A[k]
    logits_A = [0.8, 0.3, 0.1, ...]  (k=0ì´ í›„ë³´ì™€ ê°€ì¥ ê´€ë ¨ ë†’ìŒ)

  For News B:
    logits_B = [0.5, 0.6, 0.2, ...]

Softmax:
  Î±_A = [0.4, 0.2, 0.05, ...]  (k=0ì— ì§‘ì¤‘)
  Î±_B = [0.25, 0.3, 0.1, ...]  (k=1ì— ì§‘ì¤‘)

User Representation:
  u_A = 0.4 * E_A[0] + 0.2 * E_A[1] + ...
  u_B = 0.25 * E_B[0] + 0.3 * E_B[1] + ...

Shape:
  user_representation: [1, 2, 768]
```

---

**5. Click Prediction**

```
Dot Product:
  score_A = u_A Â· h_A = 12.5
  score_B = u_B Â· h_B = 8.3

Softmax:
  P(click A) = exp(12.5) / (exp(12.5) + exp(8.3)) = 0.98
  P(click B) = 0.02

Prediction: News A (sports)
```

---

### 3.5 ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜í–¥

| íŒŒë¼ë¯¸í„° | ê°’ | ì˜í–¥ |
|---------|-----|------|
| **K (num_interest_vectors)** | 32 | ë§ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ í‘œí˜„ ê°€ëŠ¥, ê³„ì‚°ëŸ‰ ì¦ê°€ |
| **context_dim** | 200 | Context codes ì°¨ì›, í´ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€ |
| **category_aware_lambda** | 0.5 | í´ìˆ˜ë¡ ì¹´í…Œê³ ë¦¬ ì˜í–¥ ì¦ê°€, 0ì´ë©´ category-agnostic |
| **disagreement_beta** | 0.8 | í´ìˆ˜ë¡ ë‹¤ì–‘ì„± ê°•ì œ, ë„ˆë¬´ í¬ë©´ click prediction ì„±ëŠ¥ ì €í•˜ |
| **plm_frozen_layers** | 10 | ë§ì„ìˆ˜ë¡ PLM íŒŒë¼ë¯¸í„° ê³ ì • (ë¹ ë¥´ì§€ë§Œ ì„±ëŠ¥â†“) |
| **plm_lr** | 1e-5 | PLM fine-tuning learning rate (ë‚®ì„ìˆ˜ë¡ ì•ˆì •) |

---

### 3.6 GloVe ì´ˆê¸°í™” vs Random ë¹„êµ

| êµ¬ë¶„ | Random ì´ˆê¸°í™” | GloVe ì´ˆê¸°í™” |
|------|--------------|-------------|
| **Category Embedding** | Uniform(-0.1, 0.1) | GloVe 840B 300d (truncate to 50) |
| **ì½”ì‚¬ì¸ ìœ ì‚¬ë„** | ë¬´ì˜ë¯¸ (random) | ì˜ë¯¸ì  ìœ ì‚¬ë„ ë°˜ì˜ |
| **Category-Aware Attention** | íš¨ê³¼ ì—†ìŒ | ì˜ë¯¸ ìˆëŠ” ê°€ì¤‘ì¹˜ ì¡°ì • |
| **í•™ìŠµ í•„ìš”ì„±** | ë†’ìŒ (scratch) | ë‚®ìŒ (frozen ê°€ëŠ¥) |
| **ì„±ëŠ¥** | ë‚®ìŒ | ë†’ìŒ (íŠ¹íˆ cold-start) |

**ì˜ˆì‹œ ë¹„êµ:**
```
Random:
  cos(sports_emb, entertainment_emb) = -0.03
  cos(sports_emb, finance_emb) = 0.12
  â†’ ë¬´ì˜ë¯¸í•œ ê°’

GloVe:
  cos(sports_emb, entertainment_emb) = 0.42
  cos(sports_emb, finance_emb) = 0.15
  â†’ ì‹¤ì œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ë°˜ì˜
```

---

## 4. í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 4.1 PLMMiner ì„¤ê³„

1. **PLM í™œìš©**: BERTë¡œ ê°•ë ¥í•œ í…ìŠ¤íŠ¸ í‘œí˜„ í•™ìŠµ
2. **Minimal Fusion**: Category/SubCategory fusion ì—†ì´ ìˆœìˆ˜ PLM ì¶œë ¥ë§Œ ë°˜í™˜ (MINERê°€ ì§ì ‘ category embedding ì‚¬ìš©)
3. **GloVe Category Embedding**: MINERì˜ category-aware attentionì„ ìœ„í•œ ì˜ë¯¸ì  ì¹´í…Œê³ ë¦¬ í‘œí˜„
4. **Frozen PLM Layers**: í•˜ìœ„ 10ê°œì¸µ ê³ ì •ìœ¼ë¡œ ê³„ì‚° íš¨ìœ¨ ë° overfitting ë°©ì§€

### 4.2 MINER ì„¤ê³„

1. **Poly Attention**: 32ê°œì˜ orthogonal context codesë¡œ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ ìº¡ì²˜
2. **Category-Aware Attention**: GloVe ì´ˆê¸°í™”ëœ ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
3. **Disagreement Regularization**: Interest vectors ë‹¤ì–‘ì„± ê°•ì œë¡œ ê´€ì‹¬ì‚¬ collapse ë°©ì§€
4. **Weighted Aggregation**: í›„ë³´ ë‰´ìŠ¤ë³„ ê´€ë ¨ interest vectorsì— ë™ì  ê°€ì¤‘ì¹˜ ë¶€ì—¬

### 4.3 í›ˆë ¨ ì „ëµ

1. **Differential Learning Rate**: PLM (1e-5) vs ê¸°íƒ€ (1e-4)
2. **LR Scheduling**: 10% warmup + linear decay
3. **Auxiliary Loss**: Click prediction + Disagreement regularization
4. **Frozen Category Embedding**: GloVe ì˜ë¯¸ì  í‘œí˜„ ë³´ì¡´

---

## 5. ì‹¤í—˜ ê¶Œì¥ì‚¬í•­

### Ablation Studies

1. **GloVe íš¨ê³¼**:
   ```bash
   # With GloVe
   --use_category_glove

   # Without GloVe (baseline)
   # (í”Œë˜ê·¸ ì œê±°)
   ```

2. **Category-Aware Attention íš¨ê³¼**:
   ```bash
   # Full category-aware
   --category_aware_lambda=0.5

   # No category-aware
   --category_aware_lambda=0.0
   ```

3. **Interest Vectors ê°œìˆ˜**:
   ```bash
   --num_interest_vectors=16  # vs 32 vs 64
   ```

4. **Aggregation ë°©ì‹**:
   ```bash
   --miner_aggregation=weighted  # vs max vs mean
   ```

---

**ì‘ì„± ì™„ë£Œ**: 2025-12-20
**ë²„ì „**: 1.0
